{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CARDS classifier with RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "\n",
    "# Dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Unidecoder\n",
    "import unicodedata\n",
    "\n",
    "# Timestamp / time measurment\n",
    "import time\n",
    "\n",
    "# Simpletransformers classifier\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "# Label encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Model performance scores\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# PyTorch: enable GPU access\n",
    "import torch\n",
    "\n",
    "# If you want to select a specific GPU, set it here:\n",
    "# gpu = 0\n",
    "# torch.cuda.set_device(gpu) \n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use GPU {}:'.format(torch.cuda.current_device()), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required functions\n",
    "\n",
    "# Define additional model performance scores (F1)\n",
    "def f1_multiclass_macro(labels, preds):\n",
    "    return f1_score(labels, preds, average='macro')\n",
    "def f1_multiclass_micro(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "def f1_multiclass_weighted(labels, preds):\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "def f1_class(labels, preds):\n",
    "    return f1_score(labels, preds, average=None)\n",
    "def precision(labels, preds):\n",
    "    return precision_score(labels, preds, average='macro')\n",
    "def recall(labels, preds):\n",
    "    return recall_score(labels, preds, average='macro')\n",
    "\n",
    "# Define text pre-processing functions\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "def strip_underscores(text):\n",
    "    return re.sub(r'_+', ' ', text)\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "# Merge text pre-processing functions\n",
    "def denoise_text(text):\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = strip_underscores(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and pre-process the text data\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('training/train.csv').rename(columns={\"label\": \"labels_orig\"})\n",
    "valid = pd.read_csv('training/valid.csv').rename(columns={\"label\": \"labels_orig\"})\n",
    "test = pd.read_csv('training/test.csv').rename(columns={\"label\": \"labels_orig\"})\n",
    "\n",
    "# Pre-process the text\n",
    "train['text'] = train['text'].astype(str).apply(denoise_text)\n",
    "valid['text'] = valid['text'].astype(str).apply(denoise_text)\n",
    "test['text'] = test['text'].astype(str).apply(denoise_text)\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "train['labels'] = label_encoder.fit_transform(train.labels_orig)\n",
    "valid['labels'] = label_encoder.fit_transform(valid.labels_orig)\n",
    "test['labels'] = label_encoder.fit_transform(test.labels_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of categories\n",
    "print(round(train.labels.value_counts(normalize=True),2))\n",
    "# Calculate weights\n",
    "weights = compute_class_weight('balanced', train.labels.unique(), train.labels)\n",
    "weights = [*weights]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('roberta', 'roberta-large', \n",
    "                            num_labels = 18, weight = weights,\n",
    "                            args={'reprocess_input_data': True, \n",
    "                                  'overwrite_output_dir': False,\n",
    "                                  'output_dir': 'models/new_model/',\n",
    "                                  'best_model_dir': 'models/new_model/best_model/',\n",
    "                                  # Hyperparameters\n",
    "                                  'train_batch_size': 6,\n",
    "                                  'num_train_epochs': 3, \n",
    "                                  'learning_rate': 1e-5,\n",
    "                                  # Text processing\n",
    "                                  'max_seq_length': 256,\n",
    "                                  'sliding_window': True,\n",
    "                                  'stride': 0.6,\n",
    "                                  'do_lower_case': False,\n",
    "                                  # Evaluation\n",
    "                                  'evaluate_during_training': True,\n",
    "                                  'evaluate_during_training_verbose': True,\n",
    "                                  'evaluate_during_training_steps': -1,\n",
    "                                  # Saving\n",
    "                                  'save_model_every_epoch': True,\n",
    "                                  'save_eval_checkpoints': True,\n",
    "                                  'weight_decay': 0\n",
    "                                  })\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train_model(train, eval_df = valid,\n",
    "                  f1_macro = f1_multiclass_macro, \n",
    "                  f1_micro = f1_multiclass_micro, \n",
    "                  f1_weighted = f1_multiclass_weighted, \n",
    "                  acc = accuracy_score, \n",
    "                  f1_class = f1_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa classifier performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate the classifier performance on the validation data\n",
    "result, model_outputs, wrong_predictions = model.eval_model(valid, \n",
    "                                                            f1_macro = f1_multiclass_macro,\n",
    "                                                            precision = precision, \n",
    "                                                            recall = recall,\n",
    "                                                            acc = accuracy_score,\n",
    "                                                            f1_micro = f1_multiclass_micro, \n",
    "                                                            f1_weighted = f1_multiclass_weighted, \n",
    "                                                            f1_class = f1_class)\n",
    "\n",
    "print('\\n\\nThese are the results when testing the model on the validation data set:\\n')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate the classifier performance on the testing data\n",
    "result_test, model_outputs_test, wrong_predictions_test = model.eval_model(test, \n",
    "                                                                           f1_macro = f1_multiclass_macro,\n",
    "                                                                           precision = precision, \n",
    "                                                                           recall = recall,\n",
    "                                                                           acc = accuracy_score,\n",
    "                                                                           f1_micro = f1_multiclass_micro, \n",
    "                                                                           f1_weighted = f1_multiclass_weighted,\n",
    "                                                                           f1_class = f1_class)\n",
    "print('\\n\\nThese are the results when testing the model on the testing data set:\\n')\n",
    "print(result_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
