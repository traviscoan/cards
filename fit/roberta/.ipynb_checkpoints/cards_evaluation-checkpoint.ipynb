{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation of the pre-trained RoBERTa classifier used in the Coan et al. (2021) article 'Computer-assisted detection and classification of misinformation about climate change'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Author: Mirjam Nanko\n",
    "## Date Created: 2021-02-01\n",
    "## Email: m.nanko@exeter.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "\n",
    "# Dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Unidecoder\n",
    "import unicodedata\n",
    "\n",
    "# Timestamp / time measurment\n",
    "import time\n",
    "\n",
    "# Simpletransformers classifier\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "# Label encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Model performance scores\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# PyTorch: enable GPU access\n",
    "import torch\n",
    "\n",
    "# If you want to select a specific GPU, set it here:\n",
    "# gpu = 0\n",
    "# torch.cuda.set_device(gpu) \n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use GPU {}:'.format(torch.cuda.current_device()), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required functions\n",
    "\n",
    "# Define additional model performance scores (F1)\n",
    "def f1_multiclass_macro(labels, preds):\n",
    "    return f1_score(labels, preds, average='macro')\n",
    "def f1_multiclass_micro(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "def f1_multiclass_weighted(labels, preds):\n",
    "    return f1_score(labels, preds, average='weighted')\n",
    "def f1_class(labels, preds):\n",
    "    return f1_score(labels, preds, average=None)\n",
    "def precision(labels, preds):\n",
    "    return precision_score(labels, preds, average='macro')\n",
    "def recall(labels, preds):\n",
    "    return recall_score(labels, preds, average='macro')\n",
    "\n",
    "# Define text pre-processing functions\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "def strip_underscores(text):\n",
    "    return re.sub(r'_+', ' ', text)\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "# Merge text pre-processing functions\n",
    "def denoise_text(text):\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = strip_underscores(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and pre-process the text data\n",
    "\n",
    "# Load the data\n",
    "valid = pd.read_csv('training/valid.csv').rename(columns={\"label\": \"labels_orig\"})\n",
    "test = pd.read_csv('training/test.csv').rename(columns={\"label\": \"labels_orig\"})\n",
    "\n",
    "# Pre-process the text\n",
    "valid['text'] = valid['text'].astype(str).apply(denoise_text)\n",
    "test['text'] = test['text'].astype(str).apply(denoise_text)\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "valid['labels'] = label_encoder.fit_transform(valid.labels_orig)\n",
    "test['labels'] = label_encoder.fit_transform(test.labels_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance of the pre-trained RoBERTa classifier used in the Coan et al. (2021) article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define model \n",
    "architecture = 'roberta'\n",
    "\n",
    "# Define trained classifier\n",
    "model_name = 'CARDS_RoBERTa_Classifier'\n",
    "\n",
    "# Load the classifier\n",
    "model = ClassificationModel(architecture, model_name)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "result, model_outputs, wrong_predictions = model.eval_model(valid, \n",
    "                                                            f1_macro = f1_multiclass_macro, \n",
    "                                                            precision = precision, \n",
    "                                                            recall = recall,\n",
    "                                                            acc = accuracy_score,\n",
    "                                                            f1_micro = f1_multiclass_micro, \n",
    "                                                            f1_weighted = f1_multiclass_weighted, \n",
    "                                                            f1_class = f1_class)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "result_test, model_outputs_test, wrong_predictions_test = model.eval_model(test, \n",
    "                                                                           f1_macro = f1_multiclass_macro,\n",
    "                                                                           precision = precision, \n",
    "                                                                           recall = recall,\n",
    "                                                                           acc = accuracy_score, \n",
    "                                                                           f1_micro = f1_multiclass_micro, \n",
    "                                                                           f1_weighted = f1_multiclass_weighted, \n",
    "                                                                           f1_class = f1_class)\n",
    "\n",
    "# Print results\n",
    "print('\\n\\nThese are the results when testing the model on the validation data set:\\n')\n",
    "print(result)\n",
    "\n",
    "print('\\n\\nThese are the results when testing the model on the testing data set:\\n')\n",
    "print(result_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
